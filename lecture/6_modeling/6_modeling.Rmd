---
title: "`r params$title`"
subtitle: "`r params$subtitle`"
author: "`r params$author`"
date: "`r format(Sys.Date(), '%d.%m.%Y')`"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "style.css"]
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
params:
  title: "Data Analysis using R"
  subtitle: "Modeling"
  author: "Sven Werenbeck-Ueding"
  path_bib: "references.bib"
  path_assets: "../../resources/assets/"
  path_data: "../../data/"
---

<!-- Setup -->

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(fontawesome)
library(DiagrammeR)
library(RefManageR)

# Theming
xaringanExtra::use_panelset()
xaringanExtra::use_tile_view() # Press o for overview in browser

# Bibliography
BibOptions(
  check.entries = FALSE,
  bib.style = "authoryear",
  style = "markdown"
)

bib <- ReadBib(params$path_bib, check = FALSE)
```

```{r eval=TRUE, child = params$path_slides}
```

```{r pdf, include = FALSE}
source("https://git.io/xaringan2pdf")
xaringan_to_pdf(
  input = "6_modeling.html",
  output_file = "6_modeling.pdf")
```

```{r echo=FALSE, out.width="100%", fig.align='center'}

grViz(
  '
  digraph {
  graph [fontname="Helvetica,Arial,sans-serif", compound = true]
	node [fontname="Helvetica,Arial,sans-serif"]
	edge [fontname="Helvetica,Arial,sans-serif"]
    
    rankdir=LR
    
    subgraph cluster_0 {
        style = rounded
        color = black
    
        {
            node [style = rounded, shape = box]
            Import -> Tidy
        }
    
        subgraph cluster_1 {
            style = rounded
            color = black
            
            node [style = rounded, shape = box]
            Transform Visualize Model
            
            edge []
            Transform -> Visualize
            Visualize -> Model
            Model -> Transform
            
            label = "Exploration"
            labeljust = "l"
            labelloc = "b"
        }
    
        {
            node [style = rounded, shape = box]
            Communicate
        }
    
        Tidy -> Transform [lhead = cluster_1]
        Model -> Communicate [ltail = cluster_1]
        
        label = "Programming"
        labeljust = "l"
        labelloc = "b"
    }
  }
  '
)

```

.right[*Source:* `r Citet(bib, "r4ds")`]

---

class: middle

## Fenced Out:

### The Impact of Border Construction on U.S.-Mexico Migration

> This paper estimates the **impact of the US-Mexico border fence
on US-Mexico migration** by **exploiting variation in the timing and
location of US government investment in fence construction**. Using
Mexican survey data and data I collected on fence construction,
I find that construction in a municipality reduces migration by
27 percent for municipality residents and 15 percent for residents of
adjacent municipalities.
>
> .right[`r Citet(bib, "f20")`]

---

layout: true

## Data

---

.pull-left[
#### Mexican Survey

- Encuesta Nacional de Ocupaci√≥n y Empleo (ENOE) from Q3 2003 to Q3 2013

- Quarterly rotating panel: Households included for 5 quarters

- Records whether any household member leaves to the US

- Potential migrants are restricted from ages 15 to 65

- Explanatory variables: age, gender, marital status and education for all household members
]

--

.pull-right[
#### Fence Construction

- Data collected by identifying potential fence locations:

  - Documents from US authorities: Customs and Border Protection (CPB) and Government Accountability Office (GAO)
  - Local government reports
  - Published contracts with construction firms

- Cross-checked with an environmental organization tracking the impact of fence construction (Sierra Club)

- Provides quarterly information on fence construction on Mexican municipality level
]

---

layout: false
name: logit-model

## Identification Strategy

- By exploiting temporal and spatial variation in fence construction, `r Citet(bib, "f20")` estimates the impact of fence construction on the migration decision of potential migrants

- Exogenous variation in fence construction allows for a difference-in-difference estimator of the form $$Pr(Y_{mti}=1\mid z_{mti})=\frac{exp(\alpha+X_{mti}\beta+\delta\times fence_{mti}+\gamma_m+\tau_t)}{1+exp(\alpha+X_{mti}\beta+\delta\times fence_{mti}+\gamma_m+\tau_t)}$$

  - $Y_{mti}\in[0,1]$: 1 if individual $i$ living in municipality $m$ migrates to the US in year-quarter $t$
  
  - $X_{mti}$: Socio-economic characteristics of individual $i$ living in municipality $m$ and year-quarter $t$
  
  - $fence_{mti}\in[0,1]$: 1 if fence construction started in municipality $m$ in or before year-quarter $t$
  
  - $\gamma_m$, $\tau_t$: Municipality and year-quarter fixed effects
  
---

## Results

.pull-left[
- Table 2 shows log-odds coefficients of different specifications for the estimation strategy

- Column (2) of panel A employs the specification on the previous slide

- Fence construction reduces probability of migrating by $1-e^\delta=`r scales::label_percent(accuracy = .01, suffix="")(1-exp(-0.476))`\%$

- Relative to baseline migration rate of $4.2$ per $1,000$ respondents

- Parentheses show standard errors clustered by municipality

- Effect is highly significant: $p$-value is $\approx0$
]

.pull-right[
```{r  out.width = "100%", fig.align='center', echo=FALSE}
knitr::include_graphics(
  paste0(params$path_assets, "feigenberg_2020_results.png")
)
```

.right[*Source:* `r Citet(bib, "f20")`]
]

---

## Prerequisites

```{r eval=FALSE}
# install.packages("tidymodels")
library(tidyverse)
library(tidymodels)

df_mig <- read_csv("data/processed/fence_migration.csv")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidymodels)

df_mig <- read_csv(
  paste0(params$path_data, "processed/fence_migration.csv")
)
```

--

.pull-left.short-width[
.big-emoji[`r emo::ji("warning")`]
]

.pull-right.long-width[
- For this course, a random 50% sample of the full sample was drawn from the data set provided by `r Citet(bib, "f20data")`

- Our results may differ
]

---

layout: false
class: middle

# Regressions in R

---

## Fit Linear Regressions in R

```{r eval=FALSE}
?lm
```

The `lm()` function fits linear models, including multivariate models. It can also be used to carry out single stratum analysis of variance and analysis of covariance.

- `formula`: An object of class `formula` that symbollically describes the model to be fitted

- `data`: An object of class `data.frame` (or coercible by `as.data.frame`) containing the model variables

- `subset`: An optional vector for subsetting observations in `data`

- `weights`: An optional numeric vector of weights, e. g. for weighted least squares

---

## Formulas in R

Expressions such as `y ~ x1 + x2 + x3` use the `~` operator to specify that response `y` is modeled by a set of predictors (`x1`, `x2` and `x3`)

| Operator | Meaning | Example |
| --- | --- | --- |
| `:` | Interaction effect between two predictors | .center[`x1:x2`] |
| `*` | Main and interaction effects of predictors | .center[`x1*x2` &rarr; `x1 + x2 + x1:x2`] |
| `^` | Expands to a formula containing main effects and interactions up to the $n^{th}$ order | .center[`(x1 + x2 + x3)^2` &rarr; <br> `x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3`] |
| `/` | Terms on the LHS are nested within those on the right | .center[`x1/x2` &rarr; `x1 + x1:x2`] |
| `-` | Removes terms from the formula (e. g. the intercept) | .center[`y ~ -1 + x1`] |
| `.` | Usually interpreted as all `data` columns not otherwise in the formula | .center[`y ~ .`] |

---

## Binary Choice Models

| Model | Functional Form | Command |
| ----- | --------------- | ------- |
| LPM | $$Pr(Y_i=1\mid X_i)=X_i\beta$$ | .center[`lm(y ~ .)`] |
| Logit | $$Pr(Y_i = 1\mid X_i)=\wedge(X_i\beta)=\frac{e^{X_i\beta}}{1+e^{X_i\beta}}$$ | .center[`glm(y ~ ., family = "binomial")`] |
| Probit | $$Pr(Y_i=1\mid X_i)=\phi(X_i\beta)=\int_{-\infty}^{X_i\beta}\phi(z)dz$$ | .center[`glm(y ~ ., binomial(link = "probit"))`] |

---

name: task-1

## Task 1: Replicate the Estimation of `r Citet(bib, "f20")`

.panelset[
.panel[.panel-name[Task]
Estimate the impact of border fence construction on Mexican-US migration. Implement the logit model employed by `r Citet(bib, "f20")` in R and interpret the results. Use the data provided in `data/processed/fence_migration.csv`.

$$Pr(Y_{mti}=1\mid z_{mti})=\wedge\left({\alpha+X_{mti}\beta+\delta\times fence_{mti}+\gamma_m+\tau_t}\right)$$

- $Y_{mti}\in[0,1]$: 1 if individual $i$ living in municipality $m$ migrates to the US in year-quarter $t$
  
- $X_{mti}$: Socio-economic characteristics of individual $i$ living in municipality $m$ and year-quarter $t$
  
- $fence_{mti}\in[0,1]$: 1 if fence construction started in municipality $m$ in or before year-quarter $t$
  
- $\gamma_m$, $\tau_t$: Municipality and year-quarter fixed effects
]
.panel[.panel-name[Code]

```{r task-1, eval=FALSE}
# Define the regression formula
full_formula <- formula(
  migrate ~ fence + female + age + educ + married + 
    as.factor(municipality) + as.factor(period)
)

# Fit a logit model as given by formula on the df_mig data set
logit_model <- glm(full_formula, df_mig, family = "binomial")

# Print a tidy model summary to the console
broom::tidy(logit_model) %>% 
  filter(term %in% c("fence", "female", "age", "educ", "married"))
```

]
.panel[.panel-name[Output]
.scroll-box-20[
```{r ref.label="task-1", echo=FALSE}
```
]
]
]

---

## Econometricians seldomly estimate just one model!

.pull-left.short-width[
.big-emoji[`r emo::ji("warning")`]
]

.pull-right.long-width[
#### Different model specifications serve different purposes

- Including additional controls for robustness checks
- Investigating heterogeneity by interacting variables
- Transforming variables, e. g. `log(y)`
]

--

.pull-left.short-width[
.big-emoji[`r emo::ji("warning")`]
]

.pull-right.long-width[
#### In some cases, specifications are estimated using different models

- Estimating logit and LPM for robustness check
- Finding the best predictor among a set of models (common in machine learning applications)
]

--

.pull-left.short-width[
.big-emoji[`r emo::ji("right arrow")`]
]

.pull-right.long-width[
#### Writing clean and easily understandable code becomes difficult

- R implementations of models have different interfaces
- Preprocessing steps are not easily interchangable using base R
]

---

class: middle
background-image: url("https://raw.githubusercontent.com/rstudio/hex-stickers/master/SVG/tidymodels.svg")
background-size: 100px
background-position: 90% 10%

# `tidymodels`

> The `tidymodels` framework is a collection of packages for modeling and machine learning using `tidyverse` principles.
>
> .right[`r Citet(bib, "tidymodels")`]

> These two aspects of model development &ndash; **ease of proper use** and **good methodological practice** &ndash; are crucial. [...] Tools should be powerful enough to create high-performance models, but, on the other hand, should be easy to use appropriately.
>
> .right[`r Citet(bib, "ks22")`]

---

## Taxonomy of Model Types

.pull-left.filled[
### `r fa("chart-column")` Descriptive

- Illustrates characteristics of the data
- E. g. visualize data to identify relationships between variables
- Discover ways to represent variables in a model
- Almost always precedes other types of models
]

--

.pull-right.filled[
### `r fa("chart-line")` Inferential

- Explores hypotheses on the relation between variables
- Produces probabilistic output to find some statistical conclusion, e. g. t-tests
- Acceptance or rejection of hypothesis depends on pre-defined assumptions
]

--

.filled[
### `r fa("arrow-trend-up")` Predictive

- Estimating new data on a pre-trained model to predict values as close as possible to the new values
- Less concerned with how variables are related in the model and more *empirically* driven
]

---

layout: false
class: middle
background-image: url("https://raw.githubusercontent.com/rstudio/hex-stickers/master/SVG/parsnip.svg")
background-size: 100px
background-position: 90% 10%

# `parsnip`

> The goal of parsnip is to provide a tidy, unified interface to models that can be used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages.
>
> .right[`r Citet(bib, "parsnip")`]

---

## Why use `parsnip`?

### 1. Separate the model definition from its evaluation
- Same model is often re-run with different preprocessing steps
- E. g. baseline specification and specification with additional controls

### 2. Detach model specification from implementation
- Different engines could be used to fit a linear model, e. g. `lm`, `glm` etc.
- Makes comparison between engines easier

### 3. Provide consistent naming of function arguments

--

.pull-left.short-width[
.big-emoji[`r emo::ji("right arrow")`]
]

.pull-right.lo-wi[
`parsnip` offers a unified approach to model specifications
]

---

## Components of Model Specifications

### 1. Model Type

Specify the model type to use (e. g. `linear_reg()` for linear regression, `logistic_reg` for logit, ...)

### 2. Engine

Set the engine for fitting the model. Run `show_engines("linear_reg")` to get a list of engines (and their modes)

### 3. Mode

When required, determine the model's mode. Numeric outcomes are estimated by use of regression, whereas qualitative outcomes require a classification model. For some models, e. g. linear regression, only one mode is available and therefore automatically set (regression).

---

layout: false
name: task-2

## Task 2: Logit Model Specification

.panelset[
.panel[.panel-name[Task]
Revisit the replication of `r Citet(bib, "f20")`'s identification strategy in [Task 1](#task-1). This time, use the `parsnip` package for specifying and fitting the model. Print a summary of your model to the console.
]
.panel[.panel-name[Code]
```{r task-2, eval=FALSE}
# Specify type and engine of the logit model
logit_model <- logistic_reg() %>% 
  set_engine("glm")

# Fit the model in a separate step using the formula
formula_full <- formula(
  as.factor(migrate) ~ fence + female + age + educ + married + 
    as.factor(municipality) + as.factor(period)
)

logit_fit_full <- logit_model %>% 
  fit(formula_full, data = df_mig)

# Retrieve the fitted glm object and print a summary
tidy(logit_fit_full) %>% 
  filter(term %in% c("fence", "female", "age", "educ", "married"))
```

]
.panel[.panel-name[Output]
```{r ref.label="task-2", echo=FALSE}
```
]
]

---

name: task-3

## Task 3: Fit Different Specification

.panelset[
.panel[.panel-name[Task]
Use the same model type and engine as in [Task 2](#task-2), but exclude the additional controls in your regression formula: $$Pr(Y_{mti}=1\mid z_{mti})=\wedge\left({\alpha+\delta\times fence_{mti}+\gamma_m+\tau_t}\right)$$
]
.panel[.panel-name[Code]

```{r task-3, eval=FALSE}
# Specify the formula without additional controls
formula_reduced <- formula(
  as.factor(migrate) ~ fence + as.factor(municipality) + as.factor(period)
)

# Fit the formula to the logit model defined before
logit_reduced_fit <- logit_model %>% 
  fit(formula_reduced, data = df_mig)

# Print a summary of the fitted glm model
tidy(logit_reduced_fit) %>% 
  filter(term == "fence")
```

]
.panel[.panel-name[Output]
```{r ref.label="task-3", echo=FALSE}
```
]
]

---

name: multiple-specifications

## Estimating Multiple Specifications

.pull-left[
- As seen in [Task 3](#task-3), `parsnip` allows for greater flexibility in modeling than `base` R

  - Various specifications can be run on the same model
  - `base` R models would have to be specified independently
  - Model engine can be easily exchanged as well

- Especially useful when estimating a multitude of different specifications

- Modeling via `parsnip` can be natively used in functional programming (see the code on the right)
]
.pull-left[
```{r eval=FALSE}
logit_model <- logistic_reg() %>% 
  set_engine("glm")

formulas <- list(
  reduced = formula(
    as.factor(migrate) ~ fence + 
      as.factor(municipality) + 
      as.factor(period)
  ),
  full = formula(
    as.factor(migrate) ~ fence + female + 
      age + educ + married + 
      as.factor(municipality) + 
      as.factor(period)
  )
)

logit_fit <- formulas %>% 
  map(~ fit(logit_model, ., data = df_mig))
```

]

---

layout: false
class: middle
background-image: url("https://raw.githubusercontent.com/rstudio/hex-stickers/master/SVG/workflows.svg")
background-size: 100px
background-position: 90% 10%

# `workflows`

> Managing both a `parsnip` model and a preprocessor, such as a model formula or recipe from `recipes`, can often be challenging. The goal of `workflows` is to streamline this process by bundling the model alongside the preprocessor, all within the same object.
>
> .right[`r Citet(bib, "workflows")`]

---

## What constitutes a model?

> **Model fitting** is often **considered to be the only step in the modeling** process. Non-trivial data structures and models with high(er) complexity, however, require additional steps:

--

.pull-left.filled[
### `r fa("gears")` Pre-Processing

- Selecting predictors from a set of candidates
  - Exploratory data analysis
  - Domain knowledge
  - Data-driven algorithms
- Imputation of missing values
- Transforming the scale of a predictor
]

--

.pull-right.filled[
### Post-Processing

- Transforming estimates into interpretable format, e. g. log-odds for logit regressions
- Adjusting standard errors
]

---

## Workflow Basics

```{r eval=FALSE}
?workflow
```

> A `workflow` is a container object that aggregates information required to fit and predict from a model. Workflows *always* require a `parsnip` model object **and** a preprocessor.

#### Arguments:

| Argument | Description |
| -------- | ----------- |
| `preprocessor` | A preprocessor used for processing the data prior to fitting the model. Can be an object of class `formula`, specifying the variables in a model. |
| `spec` | A `parsnip` model specification. |

---

## Creating a Workflow

.panelset[
.panel[.panel-name[Code]
.pull-left[
- Use the formula incl. additional controls defined on previous slides as `preprocessor`

- Set the workflow's model to the `parsnip` model object
]
.pull-right[
```{r workflow-1, eval=FALSE}
# Add preprocessor and model to a workflow
logit_full_wflow <- workflow(
  preprocessor = formula_full,
  spec = logit_model
)

# Print the workflow object
logit_full_wflow
```
]
]
.panel[.panel-name[Output]
```{r ref.label="workflow-1", echo=FALSE}
```
]
]

---

## Fitting a Workflow

.panelset[
.panel[.panel-name[Code]

.pull-left[
- `fit()` the workflow on the `df_mig` data set

- Returns a workflow object with a fitted parsnip model in the `.$fit$fit` of the workflow object
]

.pull-right[
```{r workflow-2, eval=FALSE}
# Fit the workflow to the CPR data
logit_full_fit <- fit(logit_full_wflow,
                      df_mig)

# Use the tidy command to get a tibble of
# estimated coefficients
logit_full_fit %>% 
  tidy() %>% 
  filter(
    term %in% c(
      "fence", "female", "age", "educ",
      "married"
    )
  )
```
]

]
.panel[.panel-name[Output]
```{r ref.label="workflow-2", echo=FALSE}
```
]
]

---

layout: false

class: middle
background-image: url("https://raw.githubusercontent.com/rstudio/hex-stickers/master/SVG/recipes.svg")
background-size: 100px
background-position: 90% 10%

# `recipes`

> With recipes, you can use `dplyr`-like pipeable sequences of feature engineering steps to get your data ready for modeling.
>
> .right[`r Citet(bib, "recipes")`]

---

layout: true

## Recipe Basics


```{r eval=FALSE}
?recipes::recipe
```

> A recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.

---

It defines:

#### 1. Variables

Data columns in a `data.frame` or `tbl`

#### 2. Roles

Definition of how variable are used in a model, most commonly `outcome` or `response`

#### 3. Terms

Columns in a design matrix, such as `educ` or `educ:female`. Variables with the role `predictor` are automatically main effect terms.

---

#### Arguments:

| Argument | Description |
| -------- | ----------- |
| `formula` | A model formula **without** in-line functions, e. g. `log()`. In-line functions that transform the data can be passed to the recipe using `step_*()` functions. |
| `data` | A data frame or tibble of the *template* data set. `data` does not have to be the actual data, but must have the same names and types of the target data set. For large data sets, it is sufficient to simply pass `head(data)` to the recipe. |

---

layout: false

## Creating a Recipe

.panelset[
.panel[.panel-name[Code]
.pull-left[
- Before creating the preprocessor, coerce `migrate`, `municipality` and `period` to factor (needed for estimation)

- Can not be done as in-line function in the formula
]
.pull-right[
```{r recipe-1, eval=FALSE}
# Coerce columns to factor
fct_cols <- c("migrate", "municipality",
              "period")

df_mig <- df_mig %>% 
  mutate(across(all_of(fct_cols),
                ~ forcats::as_factor(.)))

formula_full <- migrate ~ fence + female + 
  age + educ + married + 
  municipality + period

# Create a recipe by providing a formula and
# data frame for variable selection
rec_full <- recipe(formula_full, df_mig)

rec_full
```
]
]
.panel[.panel-name[Output]
```{r ref.label="recipe-1", echo=FALSE}
```
]
]

---

layout: true

## Adding Preprocessing Steps to a Recipe

---

- Recipes can be piped into step functions called `step_*()`

- Step functions are preprocessing operations on the data that can be added sequentially to a recipe *without* being immediately executed

- `dplyr`-like selector functions can be used to select columns to operate on

- Offers greater flexibility than specifying e. g. variable transformations directly in the formula

---

#### General function structure:

.pull-left[
```{r eval=FALSE}
step_*(
  recipe,
  ...
)
```
]

.pull-right[
| Argument | Description |
| -------- | ----------- |
| `recipe` | A recipe object. The step will be added to the sequence of operations for this recipe. |
| `...` | One (or more) selector functions to choose variables (see `?selections` for more details).
]

---

| Step Function | Description |
| ------------- | ----------- |
| `step_dummy()` | Converts nominal data into numeric binary model terms. |
| `step_log()` | Log transforms variables. |
| `step_interact()` | Creates new columns for interaction terms between variables. Terms for which interactions should be created are passed to the `terms` argument using a formula containing interactions or selectors. |
| `step_mutate()` | Adds variables using `dplyr::mutate()` |
| `step_filter()` | Removes rows using `dplyr::filter()` |
| `step_select()` | Selects variables using `dplyr::select()` |
| `step_naomit()` | Removes rows containing `NA` or `NaN` |

*Note:* See the [`recipes` reference](https://recipes.tidymodels.org/reference/index.html) for a complete overview of step functions included in the package.

---

layout: false

## Selector Functions for Preprocessing Steps

```{r eval=FALSE}
?recipes::selections
```

> When selecting variables or model terms in step functions, `dplyr`-like tools are used. The selector functions can choose variables based on their name, current role, data type, or any combination of these.

| Selection | Description |
| --------- | ----------- |
| `all_numeric()` | Selection based on type |
| `all_nominal()` | Selection based on type |
| `all_predictors()` | Selection based on role |
| `all_outcomes()` | Selection based on role |
| `all_numeric_predictors()` | Selection based on type and role |
| `all_nominal_predictors()` | Selection based on type and role |

*Note:* `recipes` also supports select helpers from the `tidyselect` package, such as `everything()`, `all_of()` and `any_of()`.

---

## Preprocessing the Migration Data

.panelset[
.panel[.panel-name[Code]
.pull-left[
- Assign roles to the variables

  - `migrate` is the outcome
  - As before, `fence`, some of the socio-economic characteristics, `municipality` and `period` are the explanatory variables ("predictors")
  
- When fitting the recipe, the variables are included in the estimation as defined by their roles
]
.pull-right[
```{r recipes-1, eval=FALSE}
predictors <- c("fence", "female", "age",
                "educ", "married",
                "municipality", "period")

# Create a preprocessor by specifying the
# data set and assign roles
rec_full <- recipe(df_mig) %>%
  update_role(migrate,
              new_role = "outcome") %>% 
  update_role(all_of(predictors),
              new_role = "predictor")

# Print preprocessor
rec_full
```
]
]
.panel[.panel-name[Output]
```{r ref.label="recipes-1", echo=FALSE}
```

]
]

---

## Fitting the Model with a Recipe

.panelset[
.panel[.panel-name[Code]
```{r recipes-2, eval=FALSE}
# Create a workflow with the preprocessor and the model specification
logit_full_wflow <- workflow(preprocessor = rec_full,
                             spec = logit_model)

# Fit the workflow
logit_full_fit <- fit(logit_full_wflow, data = df_mig)

# Print a model summary
tidy(logit_full_fit) %>% 
  filter(term %in% c("fence", "female", "age", "educ", "married"))
```
]
.panel[.panel-name[Output]
```{r ref.label="recipes-2", echo=FALSE}
```
]
]

---

name: task-4

## Task 4: Fit Multiple Models

.panelset[
.panel[.panel-name[Task]
Re-run the estimations from [Task 2](#task-2) and [Task 3](#task-3) but use workflows to fit your models. Additionally, make use of the functions provided in the `purrr` package.
]
.panel[.panel-name[Code]

```{r results=FALSE}
# Store recipes in a list
recs <- list(reduced = rec_full %>% 
               step_select(-age, - female, -educ, -married),
             full = rec_full)

# Specify the model
logit_model <- logistic_reg() %>% 
  set_engine("glm")

# Map the preprocessors on workflows and fit the models
models_fit <- recs %>% 
  map(~ workflow(., spec = logit_model)) %>% # Create a workflow for each list element
  map(~ fit(., data = df_mig))                # Fit each workflow stored in the list
```
]

.panel[.panel-name[Basic Specification]
```{r echo=FALSE}
tidy(models_fit$reduced) %>% 
  filter(term %in% c("fence", "female", "age", "educ", "married"))
```
]

.panel[.panel-name[Full Specification]
```{r echo=FALSE}
tidy(models_fit$full) %>% 
  filter(term %in% c("fence", "female", "age", "educ", "married"))
```
]
]

---

## Extract the Engine Fit

> The generic function `extract_fit_engine()` returns the underlying fitted model object. For a `parsnip` model with the `"lm"` engine, i. e. a `lm` object:

```{r extract}
logit_full_fit <- extract_fit_engine(models_fit$full)

class(logit_full_fit)
```

--

.pull-left.short-width[
.big-emoji[`r emo::ji("right arrow")`]
]


.pull-right.lo-wi[
Necessary when next steps in the data analysis process, e. g. creating regression tables, require objects of a certain class as an input factor
]

---

layout: true

## Regression Tables

> A variety of packages offers solutions to create off-the-shelve tables for regression output. The `msummary()` from the `modelsummary` package takes an `lm` object &ndash; or a (named) list of `lm` objects &ndash; as input and returns a customizable regression table.

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default",
  vcov = NULL,
  stars = FALSE,
  title = NULL,
  notes = NULL,
  coef_map = NULL,
  gof_map = NULL,
  ...
)
```
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models, #<<
  output = "default",
  vcov = NULL,
  stars = FALSE,
  title = NULL,
  notes = NULL,
  coef_map = NULL,
  gof_map = NULL,
  ...
)
```
]

.pull-right[
`models`

A model object, such as `lm`, or a (named) list of models
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default", #<<
  vcov = NULL,
  stars = FALSE,
  title = NULL,
  notes = NULL,
  coef_map = NULL,
  gof_map = NULL,
  ...
)
```
]

.pull-right[
`output`

If the table should be saved directly, the filename can be specified here. If the table should be customized afterwards, this argument should be set to the desired object type, e. g. `"kableExtra"`.
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default",
  vcov = NULL, #<<
  stars = FALSE,
  title = NULL,
  notes = NULL,
  coef_map = NULL,
  gof_map = NULL,
  ...
)
```
]

.pull-right[
`vcov`

Replace model standard errors with robust standard errors by setting this argument to `"HC3"` or other variants of heteroscedasticity-consistent standard errors. Including robust standard errors in the table requires the `sandwich` package.
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default",
  vcov = NULL,
  stars = FALSE, #<<
  title = NULL,
  notes = NULL,
  coef_map = NULL,
  gof_map = NULL,
  ...
)
```
]

.pull-right[
`stars`

Show stars to indicate statistical significance by passing a named numeric vector to this argument. Most commonly, this would be set to:

`c("*" = .1, "**" = .05, "***" = .01)`
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default",
  vcov = NULL,
  stars = FALSE,
  title = NULL, #<<
  notes = NULL,
  coef_map = NULL,
  gof_map = NULL,
  ...
)
```
]

.pull-right[
`title`

Title for the table given as a string.
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default",
  vcov = NULL,
  stars = FALSE,
  title = NULL,
  notes = NULL, #<<
  coef_map = NULL,
  gof_map = NULL,
  ...
)
```
]

.pull-right[
`notes`

Pass a list or vector of strings to this arguments to show notes below the table.
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default",
  vcov = NULL,
  stars = FALSE,
  title = NULL,
  notes = NULL,
  coef_map = NULL, #<<
  gof_map = NULL,
  ...
)
```
]

.pull-right[
`coef_map`

Use a named character vector to map model variable names to coefficient names, e. g. `c(female = "Female")`. Coefficients are shown in the order of the character vector. If a coefficient is not given in the vector, it will be omitted from the table.
]

---

.pull-left[
```{r eval=FALSE}
msummary(
  models,
  output = "default",
  vcov = NULL,
  stars = FALSE,
  title = NULL,
  notes = NULL,
  coef_map = NULL,
  gof_map = NULL, #<<
  ...
)
```
]

.pull-right[
`gof_map`

A character vector specifying goodness-of-fit statistics and other model information to show at the bottom of the table. Measures are reported in the order given in the vector. See `get_gof(<YOUR-MODEL>)` for a list of measures to choose from. Names of the `data.frame` correspond to measure names that have to be provided to show in the table.

If you want to use a custom name for the measures, you can pass a `data.frame` object with the columns `"raw"`, `"clean"` and `"fmt"` instead of a character vector.
]

---

layout: false

## Task 5: Create a Summary Table Showing Your Results

.panelset[
.panel[.panel-name[Task]
Use the `msummary()` function from the `modelsummary` package to create a table showing the results from the models estimated in [Task 4](#task-4).

Show only coefficients for your variable of interest and the additional controls and cluster standard errors on municipality level using the `vcovCL()` function from the `sandwich` package.
]
.panel[.panel-name[Code]
```{r summary_table, eval=FALSE}
library(modelsummary)

models_fit %>% 
  map(extract_fit_engine) %>% 
  set_names(c("(1)", "(2)")) %>%
  msummary(
    output = "kableExtra", # May also be e. g. "latex" or a filename extension such as ".txt"
    # Use sandwich package to calculate clustered vcov matrix
    vcov = function(x) sandwich::vcovCL(x, cluster = df_mig$municipality),
    stars = c("*" = .1, "**" = .05, "***" = .01),
    title = "Impact of Fence Construction on Mexian-US Migration",
    notes = "Parantheses show clustered standard errors.",
    coef_map = c(fence = "Fence Construction", age = "Age", female = "Female",
                 educ = "Years of Schooling", married = "Married"),
    gof_map = c("nobs")
  )
```
]
]

---

class: middle

```{r ref.label="summary_table", echo=FALSE, warning=FALSE}
```

---

layout: true

# References

---

```{r, results='asis', echo=FALSE}
PrintBibliography(bib, start = 1, end = 6)
```

---

```{r, results='asis', echo=FALSE}
PrintBibliography(bib, start = 7)
```
